{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# NFL Capstone:Modeling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Starting Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sqlite3\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "source": [
    "## Import datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = pd.read_csv('../data/teamstarterdraft.csv')\n",
    "yearAV = pd.read_csv('../data/teamstarterdraftAV.csv')\n",
    "week = pd.read_csv('../data/weekstarterdraft.csv')\n",
    "weekAV = pd.read_csv('../data/weekstarterdraftAV.csv')\n",
    "yearnocoach = year.drop(columns=['coach', 'offcoor', 'defcoor', 'offscheme', 'defalign'])\n",
    "yearnocoachAV = yearAV.drop(columns=['coach', 'offcoor', 'defcoor', 'offscheme', 'defalign'])"
   ]
  },
  {
   "source": [
    "## New Datasets with bucketization of major categorical variables and ordinal encoding of weeks"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekord = week.copy()\n",
    "weekAVord = weekAV.copy()\n",
    "weekord['Week'] = weekord['Week'].replace('Wild Card', 18)\n",
    "weekord['Week'] = weekord['Week'].replace('Division', 19)\n",
    "weekord['Week'] = weekord['Week'].replace('Conf. Champ.', 20)\n",
    "weekord['Week'] = weekord['Week'].replace('SuperBowl', 21)\n",
    "\n",
    "weekAVord['Week'] = weekAVord['Week'].replace('Wild Card', 18)\n",
    "weekAVord['Week'] = weekAVord['Week'].replace('Division', 19)\n",
    "weekAVord['Week'] = weekAVord['Week'].replace('Conf. Champ.', 20)\n",
    "weekAVord['Week'] = weekAVord['Week'].replace('SuperBowl', 21)\n",
    "\n",
    "weekord['Week']= weekord['Week'].astype(int)\n",
    "weekAVord['Week']= weekAVord['Week'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearbucket = year.copy()\n",
    "yearAVbucket = yearAV.copy()\n",
    "top = yearbucket['coach'].isin(yearbucket['coach'].value_counts().index[:61])\n",
    "yearbucket.loc[~top, 'coach'] = 'other'\n",
    "\n",
    "top = yearbucket['offcoor'].isin(yearbucket['offcoor'].value_counts().index[:61])\n",
    "yearbucket.loc[~top, 'offcoor'] = 'other'\n",
    "\n",
    "top = yearbucket['defcoor'].isin(yearbucket['defcoor'].value_counts().index[:61])\n",
    "yearAVbucket.loc[~top, 'defcoor'] = 'other'\n",
    "\n",
    "top = yearAVbucket['coach'].isin(yearAVbucket['coach'].value_counts().index[:61])\n",
    "yearAVbucket.loc[~top, 'coach'] = 'other'\n",
    "\n",
    "top = yearAVbucket['offcoor'].isin(yearAVbucket['offcoor'].value_counts().index[:61])\n",
    "yearAVbucket.loc[~top, 'offcoor'] = 'other'\n",
    "\n",
    "top = yearAVbucket['defcoor'].isin(yearAVbucket['defcoor'].value_counts().index[:61])\n",
    "yearAVbucket.loc[~top, 'defcoor'] = 'other'"
   ]
  },
  {
   "source": [
    "# Model Selection Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                     R2       MSE      RMSE       MAE  \\\nyear           0.003490  6.803684  2.608387  2.203176   \nyearAV         0.000286  6.996596  2.645108  2.220828   \nweek           0.027038  6.751551  2.598375  2.174834   \nweekAV         0.038667  6.563252  2.561884  2.143305   \nyearnocoach    0.010850  6.734904  2.595169  2.183651   \nyearnocoachAV  0.003437  6.907644  2.628240  2.184574   \n\n                                                  best regressor  \nyear           ElasticNet(alpha=1.0, copy_X=True, fit_interce...  \nyearAV         Lasso(alpha=0.15264179671752318, copy_X=True, ...  \nweek           Lasso(alpha=0.0009540954763499944, copy_X=True...  \nweekAV         Lasso(alpha=0.0011513953993264468, copy_X=True...  \nyearnocoach    Ridge(alpha=1.0, copy_X=True, fit_intercept=Tr...  \nyearnocoachAV  Ridge(alpha=1.0, copy_X=True, fit_intercept=Tr...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "#import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "dfdict = {'year':year, 'yearAV':yearAV, 'week':week,'weekAV':weekAV, 'yearnocoach': yearnocoach, 'yearnocoachAV':yearnocoachAV}\n",
    "\n",
    "\n",
    "dfresults= pd.DataFrame()\n",
    "for key, df in dfdict.items():\n",
    "        X = df.drop('DraftTeamSelection', axis=1)\n",
    "        y = df['DraftTeamSelection']\n",
    "        \n",
    "        categorical_features = list(X.select_dtypes(include=['category', object]).columns)\n",
    "        categorical_transformer = OneHotEncoder(sparse=True, handle_unknown='ignore')\n",
    "\n",
    "        numeric_features = list(X.select_dtypes(include=['int', 'float']).columns)\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)])\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "        reg = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('regressor', LinearRegression())])\n",
    "        search_space = [{'regressor': [LinearRegression()],\n",
    "                        'regressor__normalize': [True, False]},\n",
    "                        {'regressor':[Ridge()],\n",
    "                        'regressor__alpha': np.logspace(-4, 0, 50),\n",
    "                        'regressor__normalize': [True, False]},\n",
    "                        {'regressor': [Lasso()],\n",
    "                        'regressor__alpha': np.logspace(-4, 0, 50),\n",
    "                        'regressor__normalize': [True, False]},\n",
    "                        {'regressor': [ElasticNet()],\n",
    "                        'regressor__l1_ratio': np.linspace(0,1,30),\n",
    "                        'regressor__normalize': [True, False]},\n",
    "                        {'regressor': [RandomForestRegressor()],\n",
    "                        'regressor__n_estimators': np.logspace(2,3,20),\n",
    "                        'regressor__max_depth': np.linspace(1,10,10),\n",
    "                        'regressor__criterion': ['mse', 'mae']}]\n",
    "        \n",
    "        reg_CV = GridSearchCV(reg, search_space, cv=5, n_jobs=-1)\n",
    "        best_model = reg_CV.fit(X_train, y_train)\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        df_best_regressor = best_model.best_estimator_.get_params()['regressor']\n",
    "        R2 = r2_score(y_test, y_pred)\n",
    "        MSE = mean_squared_error(y_test, y_pred)\n",
    "        RMSE = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        MAE = mean_absolute_error(y_test, y_pred)\n",
    "        results = pd.DataFrame({\"R2\": R2, 'MSE':MSE, 'RMSE': RMSE, 'MAE':MAE, 'best regressor': df_best_regressor}, index=[key])\n",
    "        dfresults = dfresults.append(results)\n",
    "print(dfresults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "year (4261, 73) (4261, 497)\n",
      "yearAV (3140, 82) (3140, 448)\n",
      "week (71017, 34) (71017, 93)\n",
      "weekAV (52331, 43) (52331, 102)\n"
     ]
    }
   ],
   "source": [
    "dfdict = {'year':year, 'yearAV':yearAV, 'week':week,'weekAV':weekAV}\n",
    "dfresults= pd.DataFrame()\n",
    "for key, df in dfdict.items():\n",
    "        X = df.drop('DraftTeamSelection', axis=1)\n",
    "        y = df['DraftTeamSelection']\n",
    "\n",
    "        X_dummies = pd.get_dummies(X)\n",
    "        print(key, X.shape, X_dummies.shape)\n"
   ]
  },
  {
   "source": [
    "# Model Selection Regression w/ PCA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "               PCA components        R2       MSE      RMSE       MAE  \\\nyear                       50  0.006157  6.564303  2.562090  2.148639   \nyearAV                     50 -0.000032  7.133128  2.670792  2.246177   \nweek                       50  0.012887  6.832590  2.613922  2.188608   \nweekAV                     50  0.016964  6.642123  2.577232  2.155551   \nyearnocoach                50 -0.005583  7.053212  2.655788  2.221354   \nyearnocoachAV               2 -0.004378  7.119864  2.668307  2.228804   \n\n                                                  best regressor  \nyear           Ridge(alpha=1.0, copy_X=True, fit_intercept=Tr...  \nyearAV         Lasso(alpha=0.0011513953993264468, copy_X=True...  \nweek           Ridge(alpha=0.007543120063354615, copy_X=True,...  \nweekAV         Lasso(alpha=0.004291934260128779, copy_X=True,...  \nyearnocoach    Ridge(alpha=0.6866488450042998, copy_X=True, f...  \nyearnocoachAV  Ridge(alpha=0.15264179671752318, copy_X=True, ...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "#import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "dfdict = {'year':year, 'yearAV':yearAV, 'week':week,'weekAV':weekAV, 'yearnocoach': yearnocoach, 'yearnocoachAV':yearnocoachAV}\n",
    "\n",
    "\n",
    "dfresults= pd.DataFrame()\n",
    "for key, df in dfdict.items():\n",
    "        X = df.drop('DraftTeamSelection', axis=1)\n",
    "        y = df['DraftTeamSelection']\n",
    "        \n",
    "        categorical_features = list(X.select_dtypes(include=['category', object]).columns)\n",
    "        categorical_transformer = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "        numeric_features = list(X.select_dtypes(include=['int', 'float']).columns)\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)])\n",
    "        \n",
    "        pca = PCA()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "        reg = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('pca', pca),\n",
    "                                ('regressor', LinearRegression())])\n",
    "        search_space = [{'pca__n_components':[2,15,30,50,100],\n",
    "                        'regressor': [LinearRegression()],\n",
    "                        'regressor__normalize': [True, False]},\n",
    "                        {'pca__n_components':[2,15,30,50,100],\n",
    "                        'regressor':[Ridge()],\n",
    "                        'regressor__alpha': np.logspace(-4, 0, 50),\n",
    "                        'regressor__normalize': [True, False]},\n",
    "                        {'pca__n_components':[2,15,30,50,100],\n",
    "                        'regressor': [Lasso()],\n",
    "                        'regressor__alpha': np.logspace(-4, 0, 50),\n",
    "                        'regressor__normalize': [True, False]},\n",
    "                        {'pca__n_components':[2,15,30,50,100],\n",
    "                        'regressor': [ElasticNet()],\n",
    "                        'regressor__l1_ratio': np.linspace(0,1,30),\n",
    "                        'regressor__normalize': [True, False]},\n",
    "                        {'pca__n_components':[2,15,30,50,100],\n",
    "                        'regressor': [RandomForestRegressor()],\n",
    "                        'regressor__n_estimators': np.logspace(2,3,20),\n",
    "                        'regressor__max_depth': np.linspace(1,10,10),\n",
    "                        'regressor__criterion': ['mse', 'mae']}]\n",
    "        \n",
    "        reg_CV = GridSearchCV(reg, search_space, cv=5, n_jobs=-1)\n",
    "        best_model = reg_CV.fit(X_train, y_train)\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        df_best_regressor = best_model.best_estimator_.get_params()['regressor']\n",
    "        R2 = r2_score(y_test, y_pred)\n",
    "        MSE = mean_squared_error(y_test, y_pred)\n",
    "        RMSE = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        MAE = mean_absolute_error(y_test, y_pred)\n",
    "        pca_components = reg_CV.best_estimator_.named_steps['pca'].n_components\n",
    "        results = pd.DataFrame({'PCA components': pca_components, \"R2\": R2, 'MSE':MSE, 'RMSE': RMSE, 'MAE':MAE, 'best regressor': df_best_regressor}, index=[key])\n",
    "        dfresults = dfresults.append(results)\n",
    "print(dfresults)"
   ]
  },
  {
   "source": [
    "# Model Selection Regresson run on datasets with bucketized coach values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                    R2       MSE      RMSE       MAE  \\\nyearbucket    0.002520  7.142888  2.672618  2.240276   \nyearAVbucket  0.013071  6.455497  2.540767  2.151045   \nweek          0.024096  6.787724  2.605326  2.179035   \nweekAV        0.041493  6.602885  2.569608  2.149873   \n\n                                                 best regressor  \nyearbucket    Lasso(alpha=0.0016768329368110067, copy_X=True...  \nyearAVbucket  Lasso(alpha=0.0020235896477251557, copy_X=True...  \nweek          Ridge(alpha=0.04941713361323833, copy_X=True, ...  \nweekAV        Ridge(alpha=0.040949150623804234, copy_X=True,...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "#import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#this is what I changed from cell above\n",
    "dfdict = {'yearbucket':yearbucket, 'yearAVbucket':yearAVbucket, 'week':weekord,'weekAV':weekAVord}\n",
    "\n",
    "\n",
    "dfresults= pd.DataFrame()\n",
    "for key, df in dfdict.items():\n",
    "        X = df.drop('DraftTeamSelection', axis=1)\n",
    "        y = df['DraftTeamSelection']\n",
    "        categorical_features = list(df.select_dtypes(include=['category', object]).columns)\n",
    "        categorical_transformer = OneHotEncoder(sparse=True, handle_unknown='ignore')\n",
    "\n",
    "        numeric_features = list(df.select_dtypes(include=['int', 'float']).columns)\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)])\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "        reg = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('regressor', LinearRegression())])\n",
    "        search_space = [{'regressor': [LinearRegression()],\n",
    "                        'regressor__normalize': [True, False]},\n",
    "                        {'regressor':[Ridge()],\n",
    "                        'regressor__alpha': np.logspace(-4, 0, 50),\n",
    "                        'regressor__normalize': [True, False]},\n",
    "                        {'regressor': [Lasso()],\n",
    "                        'regressor__alpha': np.logspace(-4, 0, 50),\n",
    "                        'regressor__normalize': [True, False]},\n",
    "                        {'regressor': [ElasticNet()],\n",
    "                        'regressor__l1_ratio': np.linspace(0,1,30),\n",
    "                        'regressor__normalize': [True, False]},\n",
    "                        {'regressor': [RandomForestRegressor()],\n",
    "                        'regressor__n_estimators': np.logspace(2,3,20),\n",
    "                        'regressor__max_depth': np.linspace(1,10,10),\n",
    "                        'regressor__criterion': ['mse', 'mae']}]\n",
    "        \n",
    "        reg_CV = GridSearchCV(reg, search_space, cv=5, n_jobs=-1)\n",
    "        best_model = reg_CV.fit(X_train, y_train)\n",
    "        y_pred = best_model.predict(X_test)\n",
    "        \n",
    "        df_best_regressor = best_model.best_estimator_.get_params()['regressor']\n",
    "        R2 = r2_score(y_test, y_pred)\n",
    "        MSE = mean_squared_error(y_test, y_pred)\n",
    "        RMSE = mean_squared_error(y_test, y_pred, squared=False)\n",
    "        MAE = mean_absolute_error(y_test, y_pred)\n",
    "        results = pd.DataFrame({\"R2\": R2, 'MSE':MSE, 'RMSE': RMSE, 'MAE':MAE, 'best regressor':                 df_best_regressor}, index=[key])\n",
    "        dfresults = dfresults.append(results)\n",
    "print(dfresults)"
   ]
  },
  {
   "source": [
    "# Classification Model Selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.20540156361051884\n",
      "0.08477457890001086\n",
      "SVC(C=0.25, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "0.20154291224686596\n",
      "0.06761230924493578\n",
      "SVC(C=0.25, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "0.269115890083632\n",
      "0.24101193239029695\n",
      "SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "0.2688477127967574\n",
      "0.25192539054479235\n",
      "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=128,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "0.1968727789623312\n",
      "0.06507804108386216\n",
      "SVC(C=0.25, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "0.20057859209257473\n",
      "0.06702063800041053\n",
      "SVC(C=0.25, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "               PCA components  Accuracy        f1\n",
      "year                        2  0.205402  0.084775\n",
      "yearAV                     15  0.201543  0.067612\n",
      "week                       15  0.269116  0.241012\n",
      "weekAV                     15  0.268848  0.251925\n",
      "yearnocoach                30  0.196873  0.065078\n",
      "yearnocoachAV              30  0.200579  0.067021\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "dfdict = {'year':year, 'yearAV':yearAV, 'week':week,'weekAV':weekAV, 'yearnocoach': yearnocoach, 'yearnocoachAV':yearnocoachAV}\n",
    "\n",
    "\n",
    "dfresults= pd.DataFrame()\n",
    "for key, df in dfdict.items():\n",
    "        X = df.drop('DraftPosition', axis=1)\n",
    "        y = df['DraftPosition']\n",
    "        #categorical_features = list(X.select_dtypes(include=['category', object]).columns)\n",
    "        #categorical_transformer = make_pipeline(OneHotEncoder(sparse=True, handle_unknown='ignore'))\n",
    "\n",
    "        numeric_features = list(X.select_dtypes(include=['int', 'float']).columns)\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "                ('num', numeric_transformer, numeric_features)])\n",
    "                #('cat', categorical_transformer, categorical_features)])\n",
    "        pca = PCA()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "        clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('pca', pca),\n",
    "                                ('classifier', RandomForestClassifier())])\n",
    "        search_space = [{'pca__n_components':[2,15,30,50,100],\n",
    "                        'classifier': [RandomForestClassifier()],\n",
    "                        'classifier__n_estimators': [64,96,128,1000,2000],\n",
    "                        'classifier__criterion': ['gini', 'entropy']},\n",
    "                        {'pca__n_components':[2,15,30,50,100],\n",
    "                        'classifier': [SVC()],\n",
    "                        'classifier__C': [0.25, 0.50, 0.75, 1]}]\n",
    "        \n",
    "        clf_CV = GridSearchCV(clf, search_space, cv=5, n_jobs=-1)\n",
    "        best_model = clf_CV.fit(X_train, y_train)\n",
    "        y_pred = best_model.predict(X_test)\n",
    " \n",
    "        ac = accuracy_score(y_test, y_pred)\n",
    "        print(ac)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        print(f1)\n",
    "        #cm = confusion_matrix(y_test, y_pred)\n",
    "        df_best_classifier = best_model.best_estimator_.get_params()['classifier']\n",
    "        print(df_best_classifier)\n",
    "        pca_components = clf_CV.best_estimator_.named_steps['pca'].n_components\n",
    "        #,'best classifier': df_best_classifier\n",
    "        results = pd.DataFrame({'PCA components': pca_components, \"Accuracy\": ac, \"f1\" : f1}, index=[key])\n",
    "        dfresults = dfresults.append(results)\n",
    "print(dfresults)"
   ]
  },
  {
   "source": [
    "# Best dataset (week) vs dummy classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 168 candidates, totalling 840 fits\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:   13.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:   20.9s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   40.0s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   55.5s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  4.3min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  7.7min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  9.5min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed: 13.2min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed: 15.5min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed: 18.2min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed: 21.0min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 24.7min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed: 27.2min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed: 31.5min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 36.3min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed: 39.8min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed: 44.1min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed: 46.3min\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed: 49.1min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed: 52.9min\n",
      "[Parallel(n_jobs=-1)]: Done 537 tasks      | elapsed: 58.5min\n",
      "[Parallel(n_jobs=-1)]: Done 570 tasks      | elapsed: 65.7min\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed: 74.7min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 85.1min\n",
      "[Parallel(n_jobs=-1)]: Done 677 tasks      | elapsed: 97.1min\n",
      "[Parallel(n_jobs=-1)]: Done 714 tasks      | elapsed: 109.3min\n",
      "[Parallel(n_jobs=-1)]: Done 753 tasks      | elapsed: 124.2min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 141.4min\n",
      "[Parallel(n_jobs=-1)]: Done 833 tasks      | elapsed: 219.2min\n",
      "[Parallel(n_jobs=-1)]: Done 840 out of 840 | elapsed: 234.3min finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pickle\n",
    "\n",
    "\n",
    "dfdict = {'week':week}\n",
    "\n",
    "\n",
    "dfresults= pd.DataFrame()\n",
    "for key, df in dfdict.items():\n",
    "        X = df.drop('DraftPosition', axis=1)\n",
    "        y = df['DraftPosition']\n",
    "        #categorical_features = list(X.select_dtypes(include=['category', object]).columns)\n",
    "        #categorical_transformer = make_pipeline(OneHotEncoder(sparse=True, handle_unknown='ignore'))\n",
    "\n",
    "        numeric_features = list(X.select_dtypes(include=['int', 'float']).columns)\n",
    "        scalers_to_test = [StandardScaler(), MinMaxScaler()]\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', MinMaxScaler())\n",
    "        ])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "                ('num', numeric_transformer, numeric_features)])\n",
    "                #('cat', categorical_transformer, categorical_features)])\n",
    "        pca = PCA()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "        clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                ('pca', pca),\n",
    "                                ('classifier', RandomForestClassifier())])\n",
    "        search_space = [{'pca__n_components':[15,30],\n",
    "                        'classifier': [RandomForestClassifier()],\n",
    "                        'classifier__n_estimators': [64,96,128, 256],                         'classifier__max_depth': np.linspace(1,10,10),\n",
    "                        'classifier__criterion': ['gini', 'entropy']},\n",
    "                        {'pca__n_components':[15,30],\n",
    "                        'classifier': [SVC()],\n",
    "                        'classifier__C': [0.25, 1],\n",
    "                        'classifier__kernel': ['poly', 'rbf']}]\n",
    "        \n",
    "        clf_CV = GridSearchCV(clf, search_space, cv=5, n_jobs=-1, verbose=10)\n",
    "        best_model = clf_CV.fit(X_train, y_train)\n",
    "        #saving model (https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/)\n",
    "        filename = 'best_model.sav'\n",
    "        pickle.dump(best_model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-Validation best parameters:  SVC(C=1, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='poly',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False)\n",
      "Cross-validation mean test score: 0.29585346113896405\n",
      "                  score\n",
      "stratified     0.133555\n",
      "most_frequent  0.201656\n",
      "uniform        0.112306\n",
      "      PCA components  Train Accuracy  Test Accuracy  Train F1   Test F1\n",
      "week              30         0.37681       0.299667  0.352613  0.273753\n",
      "[[2725  165  147  450  767   26  104   59  283]\n",
      " [ 784  413   34  198  465   15   72   40  199]\n",
      " [ 721   86  271  194  397   14   62   27  190]\n",
      " [ 921  106   94  826  635    4   94   54  228]\n",
      " [1166  148   76  397 1629   34  106   44  231]\n",
      " [ 389   45   45  179  257   53   27   34  111]\n",
      " [ 811  127   70  227  443   20  254   31  212]\n",
      " [ 558   77   44  134  236   12   71  140  102]\n",
      " [1121  116   96  331  515   23   72   40  712]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DB       0.30      0.58      0.39      4726\n",
      "          DE       0.32      0.19      0.24      2220\n",
      "          DT       0.31      0.14      0.19      1962\n",
      "          LB       0.28      0.28      0.28      2962\n",
      "          OL       0.30      0.43      0.36      3831\n",
      "          QB       0.26      0.05      0.08      1140\n",
      "          RB       0.29      0.12      0.17      2195\n",
      "          TE       0.30      0.10      0.15      1374\n",
      "          WR       0.31      0.24      0.27      3026\n",
      "\n",
      "    accuracy                           0.30     23436\n",
      "   macro avg       0.30      0.23      0.24     23436\n",
      "weighted avg       0.30      0.30      0.27     23436\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "165     530.400788      3.799027        29.533797        0.111991   \n",
       "167     509.903861     68.741612        32.315766        8.808143   \n",
       "154      56.801730      0.824380         0.265066        0.018032   \n",
       "156      77.130545      0.974941         0.358732        0.043102   \n",
       "152      41.386025      3.784504         0.219704        0.061618   \n",
       "\n",
       "                                      param_classifier  \\\n",
       "165  SVC(C=1, break_ties=False, cache_size=200, cla...   \n",
       "167  SVC(C=1, break_ties=False, cache_size=200, cla...   \n",
       "154  RandomForestClassifier(bootstrap=True, ccp_alp...   \n",
       "156  RandomForestClassifier(bootstrap=True, ccp_alp...   \n",
       "152  RandomForestClassifier(bootstrap=True, ccp_alp...   \n",
       "\n",
       "    param_classifier__criterion param_classifier__max_depth  \\\n",
       "165                         NaN                         NaN   \n",
       "167                         NaN                         NaN   \n",
       "154                     entropy                          10   \n",
       "156                     entropy                          10   \n",
       "152                     entropy                          10   \n",
       "\n",
       "    param_classifier__n_estimators param_pca__n_components  \\\n",
       "165                            NaN                      30   \n",
       "167                            NaN                      30   \n",
       "154                             96                      15   \n",
       "156                            128                      15   \n",
       "152                             64                      15   \n",
       "\n",
       "    param_classifier__C param_classifier__kernel  \\\n",
       "165                   1                     poly   \n",
       "167                   1                      rbf   \n",
       "154                 NaN                      NaN   \n",
       "156                 NaN                      NaN   \n",
       "152                 NaN                      NaN   \n",
       "\n",
       "                                                params  split0_test_score  \\\n",
       "165  {'classifier': SVC(C=1, break_ties=False, cach...           0.292319   \n",
       "167  {'classifier': SVC(C=1, break_ties=False, cach...           0.289797   \n",
       "154  {'classifier': RandomForestClassifier(bootstra...           0.285699   \n",
       "156  {'classifier': RandomForestClassifier(bootstra...           0.282442   \n",
       "152  {'classifier': RandomForestClassifier(bootstra...           0.282232   \n",
       "\n",
       "     split1_test_score  split2_test_score  split3_test_score  \\\n",
       "165           0.297814           0.299075           0.298760   \n",
       "167           0.294136           0.293611           0.297184   \n",
       "154           0.278794           0.278373           0.288251   \n",
       "156           0.279004           0.285204           0.294662   \n",
       "152           0.275011           0.284363           0.290143   \n",
       "\n",
       "     split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "165           0.291299         0.295853        0.003344                1  \n",
       "167           0.289723         0.292890        0.002832                2  \n",
       "154           0.296238         0.285471        0.006613                3  \n",
       "156           0.285729         0.285408        0.005207                4  \n",
       "152           0.288672         0.284084        0.005359                5  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_classifier</th>\n      <th>param_classifier__criterion</th>\n      <th>param_classifier__max_depth</th>\n      <th>param_classifier__n_estimators</th>\n      <th>param_pca__n_components</th>\n      <th>param_classifier__C</th>\n      <th>param_classifier__kernel</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>165</th>\n      <td>530.400788</td>\n      <td>3.799027</td>\n      <td>29.533797</td>\n      <td>0.111991</td>\n      <td>SVC(C=1, break_ties=False, cache_size=200, cla...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>30</td>\n      <td>1</td>\n      <td>poly</td>\n      <td>{'classifier': SVC(C=1, break_ties=False, cach...</td>\n      <td>0.292319</td>\n      <td>0.297814</td>\n      <td>0.299075</td>\n      <td>0.298760</td>\n      <td>0.291299</td>\n      <td>0.295853</td>\n      <td>0.003344</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>167</th>\n      <td>509.903861</td>\n      <td>68.741612</td>\n      <td>32.315766</td>\n      <td>8.808143</td>\n      <td>SVC(C=1, break_ties=False, cache_size=200, cla...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>30</td>\n      <td>1</td>\n      <td>rbf</td>\n      <td>{'classifier': SVC(C=1, break_ties=False, cach...</td>\n      <td>0.289797</td>\n      <td>0.294136</td>\n      <td>0.293611</td>\n      <td>0.297184</td>\n      <td>0.289723</td>\n      <td>0.292890</td>\n      <td>0.002832</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>154</th>\n      <td>56.801730</td>\n      <td>0.824380</td>\n      <td>0.265066</td>\n      <td>0.018032</td>\n      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n      <td>entropy</td>\n      <td>10</td>\n      <td>96</td>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'classifier': RandomForestClassifier(bootstra...</td>\n      <td>0.285699</td>\n      <td>0.278794</td>\n      <td>0.278373</td>\n      <td>0.288251</td>\n      <td>0.296238</td>\n      <td>0.285471</td>\n      <td>0.006613</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>156</th>\n      <td>77.130545</td>\n      <td>0.974941</td>\n      <td>0.358732</td>\n      <td>0.043102</td>\n      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n      <td>entropy</td>\n      <td>10</td>\n      <td>128</td>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'classifier': RandomForestClassifier(bootstra...</td>\n      <td>0.282442</td>\n      <td>0.279004</td>\n      <td>0.285204</td>\n      <td>0.294662</td>\n      <td>0.285729</td>\n      <td>0.285408</td>\n      <td>0.005207</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>152</th>\n      <td>41.386025</td>\n      <td>3.784504</td>\n      <td>0.219704</td>\n      <td>0.061618</td>\n      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n      <td>entropy</td>\n      <td>10</td>\n      <td>64</td>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'classifier': RandomForestClassifier(bootstra...</td>\n      <td>0.282232</td>\n      <td>0.275011</td>\n      <td>0.284363</td>\n      <td>0.290143</td>\n      <td>0.288672</td>\n      <td>0.284084</td>\n      <td>0.005359</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "\"\"\"cv_results_df = pd.DataFrame(clf_CV.cv_results_)\n",
    "cv_results_df = cv_results_df.sort_values(by=['rank_test_score'])\n",
    "cv_results_df = (\n",
    "            cv_results_df\n",
    "            .set_index(cv_results_df[\"params\"].apply(\n",
    "            lambda x: \"_\".join(str(val) for val in x.values()))\n",
    "            )\n",
    "            .rename_axis('params')\n",
    "    )\n",
    "cv_results_df[\n",
    "    ['params', 'rank_test_score', 'mean_test_score', 'std_test_score']\n",
    "    ]\"\"\"\n",
    "cv_results_df = pd.DataFrame(best_model.cv_results_)\n",
    "cv_results_df = cv_results_df.sort_values(by=['rank_test_score'])\n",
    "cv_results_df[\n",
    "    ['params', 'rank_test_score', 'mean_test_score', 'std_test_score']\n",
    "    ]\n",
    "#Check how well model generalizes\n",
    "y_train_pred = best_model.predict(X_train)\n",
    "train_ac = accuracy_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "#dummy classifier\n",
    "dummy_score_dict = {}\n",
    "strategies = ['stratified', 'most_frequent', 'uniform']\n",
    "for strategy in strategies:\n",
    "        dclf = DummyClassifier(strategy=strategy)\n",
    "        dclf.fit(X_train, y_train)\n",
    "        dummy_score = dclf.score(X_test, y_test)\n",
    "        dummy_score_dict[strategy] = dummy_score\n",
    "dummy_df = pd.DataFrame.from_dict(dummy_score_dict, orient='index', columns=['score'])\n",
    "\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "#cm = confusion_matrix(y_test, y_pred)\n",
    "df_best_classifier = best_model.best_estimator_.get_params()['classifier']\n",
    "pca_components = best_model.best_estimator_.named_steps['pca'].n_components\n",
    "#,'best classifier': df_best_classifier\n",
    "results = pd.DataFrame({'PCA components': pca_components,\"Train Accuracy\": train_ac, \"Test Accuracy\": ac, \"Train F1\":train_f1, \"Test F1\" : f1}, index=[key])\n",
    "print(\"Cross-Validation best parameters: \", cv_results_df.iloc[0]['param_classifier'])\n",
    "print(\"Cross-validation mean test score:\", cv_results_df.iloc[0]['mean_test_score'])\n",
    "dfresults = dfresults.append(results)\n",
    "print(dummy_df)\n",
    "print(dfresults)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "cv_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfresults = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Cross-Validation best parameters:  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='entropy', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=96,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False)\n",
      "Cross-validation mean test score: 0.4415005660320774\n",
      "                  score\n",
      "stratified     0.131464\n",
      "most_frequent  0.201656\n",
      "uniform        0.109234\n",
      "      PCA components  Train Accuracy  Test Accuracy  Train F1   Test F1\n",
      "week              15        0.832265        0.82736  0.832153  0.827266\n",
      "[[4073   62   67  101  145   27   80   34  137]\n",
      " [ 116 1800   21   45   98   24   36   18   62]\n",
      " [ 105   29 1594   50   65   18   33   19   49]\n",
      " [ 156   53   48 2430  115   19   41   26   74]\n",
      " [ 182   59   42  120 3245   18   60   14   91]\n",
      " [  41   37   29   37   42  900   18    9   27]\n",
      " [ 117   30   44   59   84   18 1764   24   55]\n",
      " [  73   19   24   40   37   12   21 1115   33]\n",
      " [ 164   61   53   83   93   19   53   31 2469]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          DB       0.81      0.86      0.84      4726\n",
      "          DE       0.84      0.81      0.82      2220\n",
      "          DT       0.83      0.81      0.82      1962\n",
      "          LB       0.82      0.82      0.82      2962\n",
      "          OL       0.83      0.85      0.84      3831\n",
      "          QB       0.85      0.79      0.82      1140\n",
      "          RB       0.84      0.80      0.82      2195\n",
      "          TE       0.86      0.81      0.84      1374\n",
      "          WR       0.82      0.82      0.82      3026\n",
      "\n",
      "    accuracy                           0.83     23436\n",
      "   macro avg       0.83      0.82      0.83     23436\n",
      "weighted avg       0.83      0.83      0.83     23436\n",
      "\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "10     126.216762      3.148768         0.871261        0.147203   \n",
       "12     142.526362      3.419606         0.766433        0.037040   \n",
       "8       79.992735      7.473283         0.570158        0.064450   \n",
       "14     283.720571      6.574698         4.247592        1.514612   \n",
       "0       35.041457      0.947748         0.631997        0.116116   \n",
       "\n",
       "                                     param_classifier  \\\n",
       "10  RandomForestClassifier(bootstrap=True, ccp_alp...   \n",
       "12  RandomForestClassifier(bootstrap=True, ccp_alp...   \n",
       "8   RandomForestClassifier(bootstrap=True, ccp_alp...   \n",
       "14  RandomForestClassifier(bootstrap=True, ccp_alp...   \n",
       "0   RandomForestClassifier(bootstrap=True, ccp_alp...   \n",
       "\n",
       "   param_classifier__criterion param_classifier__n_estimators  \\\n",
       "10                     entropy                             96   \n",
       "12                     entropy                            128   \n",
       "8                      entropy                             64   \n",
       "14                     entropy                            256   \n",
       "0                         gini                             64   \n",
       "\n",
       "   param_pca__n_components param_classifier__C param_classifier__kernel  \\\n",
       "10                      15                 NaN                      NaN   \n",
       "12                      15                 NaN                      NaN   \n",
       "8                       15                 NaN                      NaN   \n",
       "14                      15                 NaN                      NaN   \n",
       "0                       15                 NaN                      NaN   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "10  {'classifier': RandomForestClassifier(bootstra...           0.403068   \n",
       "12  {'classifier': RandomForestClassifier(bootstra...           0.401597   \n",
       "8   {'classifier': RandomForestClassifier(bootstra...           0.403173   \n",
       "14  {'classifier': RandomForestClassifier(bootstra...           0.395713   \n",
       "0   {'classifier': RandomForestClassifier(bootstra...           0.372701   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "10           0.479088           0.455864           0.468894   \n",
       "12           0.468264           0.452921           0.461118   \n",
       "8            0.481505           0.440101           0.466898   \n",
       "14           0.476776           0.442518           0.465847   \n",
       "0            0.467633           0.440942           0.437053   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "10           0.400588         0.441501        0.033228                1  \n",
       "12           0.414145         0.439609        0.026662                2  \n",
       "8            0.401009         0.438537        0.032594                3  \n",
       "14           0.396490         0.435469        0.033996                4  \n",
       "0            0.385351         0.420736        0.035870                5  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mean_fit_time</th>\n      <th>std_fit_time</th>\n      <th>mean_score_time</th>\n      <th>std_score_time</th>\n      <th>param_classifier</th>\n      <th>param_classifier__criterion</th>\n      <th>param_classifier__n_estimators</th>\n      <th>param_pca__n_components</th>\n      <th>param_classifier__C</th>\n      <th>param_classifier__kernel</th>\n      <th>params</th>\n      <th>split0_test_score</th>\n      <th>split1_test_score</th>\n      <th>split2_test_score</th>\n      <th>split3_test_score</th>\n      <th>split4_test_score</th>\n      <th>mean_test_score</th>\n      <th>std_test_score</th>\n      <th>rank_test_score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>126.216762</td>\n      <td>3.148768</td>\n      <td>0.871261</td>\n      <td>0.147203</td>\n      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n      <td>entropy</td>\n      <td>96</td>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'classifier': RandomForestClassifier(bootstra...</td>\n      <td>0.403068</td>\n      <td>0.479088</td>\n      <td>0.455864</td>\n      <td>0.468894</td>\n      <td>0.400588</td>\n      <td>0.441501</td>\n      <td>0.033228</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>142.526362</td>\n      <td>3.419606</td>\n      <td>0.766433</td>\n      <td>0.037040</td>\n      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n      <td>entropy</td>\n      <td>128</td>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'classifier': RandomForestClassifier(bootstra...</td>\n      <td>0.401597</td>\n      <td>0.468264</td>\n      <td>0.452921</td>\n      <td>0.461118</td>\n      <td>0.414145</td>\n      <td>0.439609</td>\n      <td>0.026662</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>79.992735</td>\n      <td>7.473283</td>\n      <td>0.570158</td>\n      <td>0.064450</td>\n      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n      <td>entropy</td>\n      <td>64</td>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'classifier': RandomForestClassifier(bootstra...</td>\n      <td>0.403173</td>\n      <td>0.481505</td>\n      <td>0.440101</td>\n      <td>0.466898</td>\n      <td>0.401009</td>\n      <td>0.438537</td>\n      <td>0.032594</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>283.720571</td>\n      <td>6.574698</td>\n      <td>4.247592</td>\n      <td>1.514612</td>\n      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n      <td>entropy</td>\n      <td>256</td>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'classifier': RandomForestClassifier(bootstra...</td>\n      <td>0.395713</td>\n      <td>0.476776</td>\n      <td>0.442518</td>\n      <td>0.465847</td>\n      <td>0.396490</td>\n      <td>0.435469</td>\n      <td>0.033996</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>35.041457</td>\n      <td>0.947748</td>\n      <td>0.631997</td>\n      <td>0.116116</td>\n      <td>RandomForestClassifier(bootstrap=True, ccp_alp...</td>\n      <td>gini</td>\n      <td>64</td>\n      <td>15</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>{'classifier': RandomForestClassifier(bootstra...</td>\n      <td>0.372701</td>\n      <td>0.467633</td>\n      <td>0.440942</td>\n      <td>0.437053</td>\n      <td>0.385351</td>\n      <td>0.420736</td>\n      <td>0.035870</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "filename = 'check_best_model.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "\n",
    "cv_results_df = pd.DataFrame(loaded_model.cv_results_)\n",
    "cv_results_df = cv_results_df.sort_values(by=['rank_test_score'])\n",
    "cv_results_df[\n",
    "    ['params', 'rank_test_score', 'mean_test_score', 'std_test_score']\n",
    "    ]\n",
    "#Check how well model generalizes\n",
    "y_train_pred = loaded_model.predict(X_train)\n",
    "train_ac = accuracy_score(y_train, y_train_pred)\n",
    "train_f1 = f1_score(y_train, y_train_pred, average='weighted')\n",
    "\n",
    "#dummy classifier\n",
    "dummy_score_dict = {}\n",
    "strategies = ['stratified', 'most_frequent', 'uniform']\n",
    "for strategy in strategies:\n",
    "        dclf = DummyClassifier(strategy=strategy)\n",
    "        dclf.fit(X_train, y_train)\n",
    "        dummy_score = dclf.score(X_test, y_test)\n",
    "        dummy_score_dict[strategy] = dummy_score\n",
    "dummy_df = pd.DataFrame.from_dict(dummy_score_dict, orient='index', columns=['score'])\n",
    "\n",
    "ac = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "#cm = confusion_matrix(y_test, y_pred)\n",
    "df_best_classifier = loaded_model.best_estimator_.get_params()['classifier']\n",
    "pca_components = loaded_model.best_estimator_.named_steps['pca'].n_components\n",
    "#,'best classifier': df_best_classifier\n",
    "results = pd.DataFrame({'PCA components': pca_components,\"Train Accuracy\": train_ac, \"Test Accuracy\": ac, \"Train F1\":train_f1, \"Test F1\" : f1}, index=[key])\n",
    "print(\"Cross-Validation best parameters: \", cv_results_df.iloc[0]['param_classifier'])\n",
    "print(\"Cross-validation mean test score:\", cv_results_df.iloc[0]['mean_test_score'])\n",
    "dfresults = dfresults.append(results)\n",
    "print(dummy_df)\n",
    "print(dfresults)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "cv_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}